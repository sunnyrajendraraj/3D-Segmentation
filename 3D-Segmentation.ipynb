{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f64463-09f6-496c-b57d-4aaf762ff681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 50\n",
      "Total number of labels: 50\n",
      "Processing and resampling complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# Assuming your images and labels are in the following directories\n",
    "images_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\images'  # Update with your actual path\n",
    "labels_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\labels'  # Update with your actual path\n",
    "\n",
    "# Resampled directories\n",
    "resampled_images_dir = 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_images'\n",
    "resampled_labels_dir = 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_labels'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(resampled_images_dir, exist_ok=True)\n",
    "os.makedirs(resampled_labels_dir, exist_ok=True)\n",
    "\n",
    "# List all files in the directories\n",
    "image_files = sorted(os.listdir(images_path))\n",
    "label_files = sorted(os.listdir(labels_path))\n",
    "\n",
    "print(f\"Total number of images: {len(image_files)}\")\n",
    "print(f\"Total number of labels: {len(label_files)}\")\n",
    "\n",
    "# Define the function to extract and remap labels\n",
    "def extract_and_remap_labels(labels):\n",
    "    new_labels = torch.zeros_like(labels)\n",
    "    new_labels[labels == 1] = 1  # Liver\n",
    "    new_labels[labels == 2] = 2  # Right Kidney\n",
    "    new_labels[labels == 13] = 3  # Left Kidney\n",
    "    new_labels[labels == 3] = 4  # Spleen\n",
    "    return new_labels\n",
    "\n",
    "# Function to resample images and labels\n",
    "def resample_image(image, target_shape=(128, 128, 128), is_label=False):\n",
    "    original_spacing = image.GetSpacing()\n",
    "    original_size = image.GetSize()\n",
    "    target_spacing = [\n",
    "        (original_size[i] * original_spacing[i]) / target_shape[i] for i in range(3)\n",
    "    ]\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(target_spacing)\n",
    "    resample.SetSize(target_shape)\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetOutputOrigin(image.GetOrigin())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "    return resample.Execute(image)\n",
    "\n",
    "# Process each image and label pair\n",
    "for img_file, lbl_file in zip(image_files, label_files):\n",
    "    img_path = os.path.join(images_path, img_file)\n",
    "    lbl_path = os.path.join(labels_path, lbl_file)\n",
    "\n",
    "    image = sitk.ReadImage(img_path)\n",
    "    label = sitk.ReadImage(lbl_path)\n",
    "\n",
    "    label_tensor = torch.tensor(sitk.GetArrayFromImage(label).astype(int))\n",
    "    remapped_labels_tensor = extract_and_remap_labels(label_tensor)\n",
    "\n",
    "    remapped_label_image = sitk.GetImageFromArray(remapped_labels_tensor.numpy())\n",
    "    remapped_label_image.CopyInformation(label)\n",
    "\n",
    "    resampled_image = resample_image(image, target_shape=(128, 128, 128), is_label=False)\n",
    "    resampled_label = resample_image(remapped_label_image, target_shape=(128, 128, 128), is_label=True)\n",
    "\n",
    "    sitk.WriteImage(resampled_image, os.path.join(resampled_images_dir, img_file))\n",
    "    sitk.WriteImage(resampled_label, os.path.join(resampled_labels_dir, lbl_file))\n",
    "\n",
    "print(\"Processing and resampling complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a957c6-7d6a-4eec-ab99-439ee829e45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 50\n",
      "Total number of labels: 50\n"
     ]
    }
   ],
   "source": [
    "# Assuming your images and labels are in the following directories\n",
    "re_images_path ='F:\\\\Repositories\\\\FLARE22Train\\\\resampled_images' # Update with your actual path\n",
    "re_labels_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_labels' # Update with your actual path\n",
    "\n",
    "# List all files in the directories\n",
    "image_files = sorted(os.listdir(images_path))\n",
    "label_files = sorted(os.listdir(labels_path))\n",
    "\n",
    "print(f\"Total number of images: {len(image_files)}\")\n",
    "print(f\"Total number of labels: {len(label_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b409248-182c-48f2-ac24-6534a7ac9b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Image File      Image Shape              Label File  \\\n",
      "0   FLARE22_Tr_0001_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0001.nii.gz   \n",
      "1   FLARE22_Tr_0002_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0002.nii.gz   \n",
      "2   FLARE22_Tr_0003_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0003.nii.gz   \n",
      "3   FLARE22_Tr_0004_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0004.nii.gz   \n",
      "4   FLARE22_Tr_0005_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0005.nii.gz   \n",
      "5   FLARE22_Tr_0006_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0006.nii.gz   \n",
      "6   FLARE22_Tr_0007_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0007.nii.gz   \n",
      "7   FLARE22_Tr_0008_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0008.nii.gz   \n",
      "8   FLARE22_Tr_0009_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0009.nii.gz   \n",
      "9   FLARE22_Tr_0010_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0010.nii.gz   \n",
      "10  FLARE22_Tr_0011_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0011.nii.gz   \n",
      "11  FLARE22_Tr_0012_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0012.nii.gz   \n",
      "12  FLARE22_Tr_0013_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0013.nii.gz   \n",
      "13  FLARE22_Tr_0014_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0014.nii.gz   \n",
      "14  FLARE22_Tr_0015_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0015.nii.gz   \n",
      "15  FLARE22_Tr_0016_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0016.nii.gz   \n",
      "16  FLARE22_Tr_0017_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0017.nii.gz   \n",
      "17  FLARE22_Tr_0018_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0018.nii.gz   \n",
      "18  FLARE22_Tr_0019_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0019.nii.gz   \n",
      "19  FLARE22_Tr_0020_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0020.nii.gz   \n",
      "20  FLARE22_Tr_0021_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0021.nii.gz   \n",
      "21  FLARE22_Tr_0022_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0022.nii.gz   \n",
      "22  FLARE22_Tr_0023_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0023.nii.gz   \n",
      "23  FLARE22_Tr_0024_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0024.nii.gz   \n",
      "24  FLARE22_Tr_0025_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0025.nii.gz   \n",
      "25  FLARE22_Tr_0026_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0026.nii.gz   \n",
      "26  FLARE22_Tr_0027_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0027.nii.gz   \n",
      "27  FLARE22_Tr_0028_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0028.nii.gz   \n",
      "28  FLARE22_Tr_0029_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0029.nii.gz   \n",
      "29  FLARE22_Tr_0030_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0030.nii.gz   \n",
      "30  FLARE22_Tr_0031_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0031.nii.gz   \n",
      "31  FLARE22_Tr_0032_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0032.nii.gz   \n",
      "32  FLARE22_Tr_0033_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0033.nii.gz   \n",
      "33  FLARE22_Tr_0034_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0034.nii.gz   \n",
      "34  FLARE22_Tr_0035_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0035.nii.gz   \n",
      "35  FLARE22_Tr_0036_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0036.nii.gz   \n",
      "36  FLARE22_Tr_0037_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0037.nii.gz   \n",
      "37  FLARE22_Tr_0038_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0038.nii.gz   \n",
      "38  FLARE22_Tr_0039_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0039.nii.gz   \n",
      "39  FLARE22_Tr_0040_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0040.nii.gz   \n",
      "40  FLARE22_Tr_0041_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0041.nii.gz   \n",
      "41  FLARE22_Tr_0042_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0042.nii.gz   \n",
      "42  FLARE22_Tr_0043_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0043.nii.gz   \n",
      "43  FLARE22_Tr_0044_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0044.nii.gz   \n",
      "44  FLARE22_Tr_0045_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0045.nii.gz   \n",
      "45  FLARE22_Tr_0046_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0046.nii.gz   \n",
      "46  FLARE22_Tr_0047_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0047.nii.gz   \n",
      "47  FLARE22_Tr_0048_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0048.nii.gz   \n",
      "48  FLARE22_Tr_0049_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0049.nii.gz   \n",
      "49  FLARE22_Tr_0050_0000.nii.gz  (128, 128, 128)  FLARE22_Tr_0050.nii.gz   \n",
      "\n",
      "        Label Shape  \n",
      "0   (128, 128, 128)  \n",
      "1   (128, 128, 128)  \n",
      "2   (128, 128, 128)  \n",
      "3   (128, 128, 128)  \n",
      "4   (128, 128, 128)  \n",
      "5   (128, 128, 128)  \n",
      "6   (128, 128, 128)  \n",
      "7   (128, 128, 128)  \n",
      "8   (128, 128, 128)  \n",
      "9   (128, 128, 128)  \n",
      "10  (128, 128, 128)  \n",
      "11  (128, 128, 128)  \n",
      "12  (128, 128, 128)  \n",
      "13  (128, 128, 128)  \n",
      "14  (128, 128, 128)  \n",
      "15  (128, 128, 128)  \n",
      "16  (128, 128, 128)  \n",
      "17  (128, 128, 128)  \n",
      "18  (128, 128, 128)  \n",
      "19  (128, 128, 128)  \n",
      "20  (128, 128, 128)  \n",
      "21  (128, 128, 128)  \n",
      "22  (128, 128, 128)  \n",
      "23  (128, 128, 128)  \n",
      "24  (128, 128, 128)  \n",
      "25  (128, 128, 128)  \n",
      "26  (128, 128, 128)  \n",
      "27  (128, 128, 128)  \n",
      "28  (128, 128, 128)  \n",
      "29  (128, 128, 128)  \n",
      "30  (128, 128, 128)  \n",
      "31  (128, 128, 128)  \n",
      "32  (128, 128, 128)  \n",
      "33  (128, 128, 128)  \n",
      "34  (128, 128, 128)  \n",
      "35  (128, 128, 128)  \n",
      "36  (128, 128, 128)  \n",
      "37  (128, 128, 128)  \n",
      "38  (128, 128, 128)  \n",
      "39  (128, 128, 128)  \n",
      "40  (128, 128, 128)  \n",
      "41  (128, 128, 128)  \n",
      "42  (128, 128, 128)  \n",
      "43  (128, 128, 128)  \n",
      "44  (128, 128, 128)  \n",
      "45  (128, 128, 128)  \n",
      "46  (128, 128, 128)  \n",
      "47  (128, 128, 128)  \n",
      "48  (128, 128, 128)  \n",
      "49  (128, 128, 128)  \n",
      "\n",
      "Unique Image Shapes and Their Counts:\n",
      "Image Shape\n",
      "(128, 128, 128)    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique Label Shapes and Their Counts:\n",
      "Label Shape\n",
      "(128, 128, 128)    50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "# Assuming your images and labels are in the following directories\n",
    "re_images_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_images'  # Update with your actual path\n",
    "re_labels_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_labels'  # Update with your actual path\n",
    "\n",
    "# List all files in the directories\n",
    "image_files = sorted(os.listdir(re_images_path))\n",
    "label_files = sorted(os.listdir(re_labels_path))\n",
    "\n",
    "# Create lists to store the dimensions of images and labels\n",
    "image_shapes = []\n",
    "label_shapes = []\n",
    "\n",
    "# Loop through all image and label files to get their dimensions\n",
    "for img_file, lbl_file in zip(image_files, label_files):\n",
    "    img_path = os.path.join(re_images_path, img_file)\n",
    "    lbl_path = os.path.join(re_labels_path, lbl_file)\n",
    "\n",
    "    # Load the NIfTI files\n",
    "    img = nib.load(img_path).get_fdata()\n",
    "    lbl = nib.load(lbl_path).get_fdata()\n",
    "\n",
    "    # Store the shapes\n",
    "    image_shapes.append(img.shape)\n",
    "    label_shapes.append(lbl.shape)\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "re_df = pd.DataFrame({\n",
    "    'Image File': image_files,\n",
    "    'Image Shape': image_shapes,\n",
    "    'Label File': label_files,\n",
    "    'Label Shape': label_shapes\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(re_df)\n",
    "\n",
    "# Get the unique shapes for images and labels\n",
    "unique_image_shapes = re_df['Image Shape'].value_counts()\n",
    "unique_label_shapes = re_df['Label Shape'].value_counts()\n",
    "\n",
    "print(\"\\nUnique Image Shapes and Their Counts:\")\n",
    "print(unique_image_shapes)\n",
    "\n",
    "print(\"\\nUnique Label Shapes and Their Counts:\")\n",
    "print(unique_label_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ec394d-fc7b-4050-9de4-d57ee842cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "\n",
    "def normalize_ct_scan(image, lower_bound=-1000, upper_bound=400):\n",
    "    \"\"\"\n",
    "    Normalize the intensity values of a CT scan by clipping and scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - image: SimpleITK image object.\n",
    "    - lower_bound: Minimum HU value to clip.\n",
    "    - upper_bound: Maximum HU value to clip.\n",
    "\n",
    "    Returns:\n",
    "    - Normalized SimpleITK image object.\n",
    "    \"\"\"\n",
    "    # Convert the SimpleITK image to a NumPy array\n",
    "    image_array = sitk.GetArrayFromImage(image)\n",
    "\n",
    "    # Clip the intensity values\n",
    "    image_array = np.clip(image_array, lower_bound, upper_bound)\n",
    "\n",
    "    # Normalize the values to the range [0, 1]\n",
    "    image_array = (image_array - lower_bound) / (upper_bound - lower_bound)\n",
    "\n",
    "    # Convert back to SimpleITK image\n",
    "    normalized_image = sitk.GetImageFromArray(image_array)\n",
    "\n",
    "    # Copy the metadata from the original image\n",
    "    normalized_image.CopyInformation(image)\n",
    "\n",
    "    return normalized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d6b8f19-09db-4a67-8d6a-d69c551fdd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization of resampled CT scans complete.\n"
     ]
    }
   ],
   "source": [
    "# Directories where resampled data is stored\n",
    "resampled_images_path ='F:\\\\Repositories\\\\FLARE22Train\\\\resampled_images'\n",
    "normalized_images_path ='F:\\\\Repositories\\\\FLARE22Train\\\\normalised_images'\n",
    "\n",
    "os.makedirs(normalized_images_path, exist_ok=True)\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(resampled_images_path, img_file)\n",
    "\n",
    "    # Load the resampled image\n",
    "    img = sitk.ReadImage(img_path)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_img = normalize_ct_scan(img)\n",
    "\n",
    "    # Save the normalized image\n",
    "    sitk.WriteImage(normalized_img, os.path.join(normalized_images_path, img_file))\n",
    "\n",
    "print(\"Normalization of resampled CT scans complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ac69c7-81ac-49a1-b8e4-25d13e5861b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 50\n"
     ]
    }
   ],
   "source": [
    "# Assuming your images and labels are in the following directories\n",
    "nprm_images_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\normalised_images' # Update with your actual path\n",
    "# re_labels_path = '/content/drive/My Drive/FLARE22Train/resampled_labels/'  # Update with your actual path\n",
    "\n",
    "# List all files in the directories\n",
    "image_files = sorted(os.listdir(images_path))\n",
    "# label_files = sorted(os.listdir(labels_path))\n",
    "\n",
    "print(f\"Total number of images: {len(image_files)}\")\n",
    "# print(f\"Total number of labels: {len(label_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eaaf085-7861-45ed-add5-8c83f0034084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 35 images\n",
      "Validation set: 7 images\n",
      "Test set: 8 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to your normalized images and labels\n",
    "normalized_images_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\normalised_images'\n",
    "labels_path = 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_labels'\n",
    "\n",
    "# List all files in the directories\n",
    "image_files = sorted(os.listdir(normalized_images_path))\n",
    "label_files = sorted(os.listdir(labels_path))\n",
    "\n",
    "# Print filenames to identify mismatches\n",
    "for img_file, lbl_file in zip(image_files, label_files):\n",
    "    # Remove the '_0000' suffix from image filenames\n",
    "    expected_lbl_file = img_file.rsplit('_', 1)[0] + '.nii.gz'\n",
    "    if expected_lbl_file != lbl_file:\n",
    "        print(f\"Mismatch: {img_file} -> {expected_lbl_file} (expected) vs {lbl_file} (actual)\")\n",
    "\n",
    "# Ensure image files match label files\n",
    "assert len(image_files) == len(label_files)\n",
    "assert all([img_file.rsplit('_', 1)[0] + '.nii.gz' == lbl_file for img_file, lbl_file in zip(image_files, label_files)])\n",
    "\n",
    "# Split the dataset into training (70%), validation (15%), and test (15%) sets\n",
    "train_imgs, test_imgs, train_lbls, test_lbls = train_test_split(image_files, label_files, test_size=0.3, random_state=42)\n",
    "val_imgs, test_imgs, val_lbls, test_lbls = train_test_split(test_imgs, test_imgs, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_imgs)} images\")\n",
    "print(f\"Validation set: {len(val_imgs)} images\")\n",
    "print(f\"Test set: {len(test_imgs)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0ffb3bd-9d15-4f64-9cd6-d9bb9c5bb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import SimpleITK as sitk\n",
    "\n",
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, image_files, label_files, image_dir, label_dir, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.label_files = label_files\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and label\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        lbl_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "\n",
    "        image = sitk.ReadImage(img_path)\n",
    "        label = sitk.ReadImage(lbl_path)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        image = sitk.GetArrayFromImage(image).astype(np.float32)\n",
    "        label = sitk.GetArrayFromImage(label).astype(np.int64)\n",
    "\n",
    "        # Optional: Apply additional transformations here\n",
    "        if self.transform:\n",
    "            # Add your transform logic here\n",
    "            image, label = self.transform(image, label)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        image = torch.from_numpy(image).unsqueeze(0)  # Add channel dimension\n",
    "        label = torch.from_numpy(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Example usage\n",
    "# Define directories and file lists\n",
    "train_dataset = CTScanDataset(train_imgs, train_lbls, normalized_images_path, labels_path)\n",
    "val_dataset = CTScanDataset(val_imgs, val_lbls, normalized_images_path, labels_path)\n",
    "test_dataset = CTScanDataset(test_imgs, test_lbls, normalized_images_path, labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea89c775-32b1-49c1-8b90-6414309c1934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Image batch shape: torch.Size([1, 1, 128, 128, 128])\n",
      "Label batch shape: torch.Size([1, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1  # Choose based on your CPU memory capacity\n",
    "num_workers = 0  # Set to 0 for debugging (loads data in the main process)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Example: Checking a batch from the training DataLoader\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    try:\n",
    "        # Ensure tensors are on the CPU\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        \n",
    "        print(f\"Batch {i+1}:\")\n",
    "        print(f\"Image batch shape: {images.shape}\")\n",
    "        print(f\"Label batch shape: {labels.shape}\")\n",
    "        break  # Only check the first batch\n",
    "    except Exception as e:\n",
    "        print(f\"Error at batch {i+1}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e74ded14-9022-44a9-a985-2be3b6229477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VNet(\n",
      "  (enc1): Sequential(\n",
      "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (enc2): Sequential(\n",
      "    (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (enc3): Sequential(\n",
      "    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (enc4): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv4): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  (dec4): Sequential(\n",
      "    (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  (dec3): Sequential(\n",
      "    (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv2): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  (dec2): Sequential(\n",
      "    (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv1): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  (dec1): Sequential(\n",
      "    (0): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (out_conv): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      ")\n",
      "Output shape: torch.Size([1, 4, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(VNet, self).__init__()\n",
    "\n",
    "        # Define the encoder (downsampling path)\n",
    "        self.enc1 = self.conv_block(1, 16)\n",
    "        self.enc2 = self.conv_block(16, 32)\n",
    "        self.enc3 = self.conv_block(32, 64)\n",
    "        self.enc4 = self.conv_block(64, 128)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(128, 256)\n",
    "\n",
    "        # Define the decoder (upsampling path)\n",
    "        self.upconv4 = self.upconv(256, 128)\n",
    "        self.dec4 = self.conv_block(256, 128)\n",
    "        self.upconv3 = self.upconv(128, 64)\n",
    "        self.dec3 = self.conv_block(128, 64)\n",
    "        self.upconv2 = self.upconv(64, 32)\n",
    "        self.dec2 = self.conv_block(64, 32)\n",
    "        self.upconv1 = self.upconv(32, 16)\n",
    "        self.dec1 = self.conv_block(32, 16)\n",
    "\n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv3d(16, num_classes, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool3d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool3d(e2, 2))\n",
    "        e4 = self.enc4(F.max_pool3d(e3, 2))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(F.max_pool3d(e4, 2))\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.upconv4(b)\n",
    "        d4 = torch.cat((d4, e4), dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e3), dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e2), dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat((d1, e1), dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # Output\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "# Check if CUDA is available; if not, use CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = VNet(num_classes=4).to(device)\n",
    "\n",
    "# Example of model summary (optional)\n",
    "print(model)\n",
    "\n",
    "# Assuming you have some input data\n",
    "input_data = torch.randn(1, 1, 64, 64, 64).to(device)  # Example input tensor\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8597d569-d588-4ce9-a13d-f4ffbc3c828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n",
      "Epoch 1/50, Training Loss: 2.1365\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown in SimpleITK ImageFileReader_Execute: D:\\a\\1\\sitk\\Code\\IO\\src\\sitkImageReaderBase.cxx:97:\nsitk::ERROR: The file \"F:\\Repositories\\FLARE22Train\\resampled_labels\\FLARE22_Tr_0004_0000.nii.gz\" does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10792\\2366193853.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;31m# Validation step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Set the model to evaluation mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Move to CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# Check and adjust labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__getitems__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__getitems__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10792\\1892989307.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlbl_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlbl_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Convert to numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetArrayFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\SimpleITK\\extra.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fileName, outputPixelType, imageIO)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetFileNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetImageIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageIO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetOutputPixelType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputPixelType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\SimpleITK\\SimpleITK.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   8426\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msame\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpixel\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mitk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mConvertPixelBuffer\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpixels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8429\u001b[0m         \"\"\"\n\u001b[1;32m-> 8430\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_SimpleITK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFileReader_Execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception thrown in SimpleITK ImageFileReader_Execute: D:\\a\\1\\sitk\\Code\\IO\\src\\sitkImageReaderBase.cxx:97:\nsitk::ERROR: The file \"F:\\Repositories\\FLARE22Train\\resampled_labels\\FLARE22_Tr_0004_0000.nii.gz\" does not exist."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the VNet model\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(VNet, self).__init__()\n",
    "\n",
    "        # Define the encoder (downsampling path)\n",
    "        self.enc1 = self.conv_block(1, 16)\n",
    "        self.enc2 = self.conv_block(16, 32)\n",
    "        self.enc3 = self.conv_block(32, 64)\n",
    "        self.enc4 = self.conv_block(64, 128)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(128, 256)\n",
    "\n",
    "        # Define the decoder (upsampling path)\n",
    "        self.upconv4 = self.upconv(256, 128)\n",
    "        self.dec4 = self.conv_block(256, 128)\n",
    "        self.upconv3 = self.upconv(128, 64)\n",
    "        self.dec3 = self.conv_block(128, 64)\n",
    "        self.upconv2 = self.upconv(64, 32)\n",
    "        self.dec2 = self.conv_block(64, 32)\n",
    "        self.upconv1 = self.upconv(32, 16)\n",
    "        self.dec1 = self.conv_block(32, 16)\n",
    "\n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv3d(16, num_classes, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool3d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool3d(e2, 2))\n",
    "        e4 = self.enc4(F.max_pool3d(e3, 2))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(F.max_pool3d(e4, 2))\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.upconv4(b)\n",
    "        d4 = torch.cat((d4, e4), dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e3), dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e2), dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat((d1, e1), dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # Output\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "# Define the Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=outputs.shape[1]).permute(0, 4, 1, 2, 3).float()\n",
    "        \n",
    "        intersection = (outputs * targets_one_hot).sum(dim=(2, 3, 4))\n",
    "        union = outputs.sum(dim=(2, 3, 4)) + targets_one_hot.sum(dim=(2, 3, 4))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "# Initialize the model\n",
    "model = VNet(num_classes=4)\n",
    "\n",
    "# Define the Cross-Entropy Loss and Dice Loss\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "dice_loss = DiceLoss()\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Learning rate scheduler to reduce the learning rate when a metric has stopped improving\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Number of epochs to train\n",
    "num_epochs = 50\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# Best validation loss for checkpointing\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "def check_and_adjust_labels(labels, num_classes):\n",
    "    min_label = labels.min().item()\n",
    "    max_label = labels.max().item()\n",
    "\n",
    "    if min_label < 0 or max_label >= num_classes:\n",
    "        print(f\"Original label range: min {min_label}, max {max_label}\")\n",
    "\n",
    "        # Adjust labels if necessary\n",
    "        labels = torch.clamp(labels, min=0, max=num_classes - 1)\n",
    "        print(f\"Adjusted label range: min {labels.min().item()}, max {labels.max().item()}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cpu(), labels.cpu()  # Move to CPU\n",
    "\n",
    "        # Check and adjust labels\n",
    "        labels = check_and_adjust_labels(labels, num_classes=4)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the losses\n",
    "        try:\n",
    "            loss_dice = dice_loss(outputs, labels)\n",
    "            loss_ce = cross_entropy_loss(outputs, labels)\n",
    "            loss = loss_dice + loss_ce\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError: {e}\")\n",
    "            print(f\"Output shape: {outputs.shape}\")\n",
    "            print(f\"Label shape: {labels.shape}\")\n",
    "            raise e\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_loss_list.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.cpu(), val_labels.cpu()  # Move to CPU\n",
    "\n",
    "            # Check and adjust labels\n",
    "            val_labels = check_and_adjust_labels(val_labels, num_classes=4)\n",
    "            \n",
    "            val_outputs = model(val_images)\n",
    "\n",
    "            val_loss_dice = dice_loss(val_outputs, val_labels)\n",
    "            val_loss_ce = cross_entropy_loss(val_outputs, val_labels)\n",
    "            val_loss += (val_loss_dice + val_loss_ce).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd692363-952c-4b29-aa63-8a27cd90bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label range: min 0, max 4\n",
      "Adjusted label range: min 0, max 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the VNet model\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(VNet, self).__init__()\n",
    "\n",
    "        # Define the encoder (downsampling path)\n",
    "        self.enc1 = self.conv_block(1, 16)\n",
    "        self.enc2 = self.conv_block(16, 32)\n",
    "        self.enc3 = self.conv_block(32, 64)\n",
    "        self.enc4 = self.conv_block(64, 128)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(128, 256)\n",
    "\n",
    "        # Define the decoder (upsampling path)\n",
    "        self.upconv4 = self.upconv(256, 128)\n",
    "        self.dec4 = self.conv_block(256, 128)\n",
    "        self.upconv3 = self.upconv(128, 64)\n",
    "        self.dec3 = self.conv_block(128, 64)\n",
    "        self.upconv2 = self.upconv(64, 32)\n",
    "        self.dec2 = self.conv_block(64, 32)\n",
    "        self.upconv1 = self.upconv(32, 16)\n",
    "        self.dec1 = self.conv_block(32, 16)\n",
    "\n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv3d(16, num_classes, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool3d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool3d(e2, 2))\n",
    "        e4 = self.enc4(F.max_pool3d(e3, 2))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(F.max_pool3d(e4, 2))\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.upconv4(b)\n",
    "        d4 = torch.cat((d4, e4), dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e3), dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e2), dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat((d1, e1), dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        # Output\n",
    "        out = self.out_conv(d1)\n",
    "        return out\n",
    "\n",
    "# Define the Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=outputs.shape[1]).permute(0, 4, 1, 2, 3).float()\n",
    "        \n",
    "        intersection = (outputs * targets_one_hot).sum(dim=(2, 3, 4))\n",
    "        union = outputs.sum(dim=(2, 3, 4)) + targets_one_hot.sum(dim=(2, 3, 4))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "# Initialize the model\n",
    "model = VNet(num_classes=4)\n",
    "\n",
    "# Define the Cross-Entropy Loss and Dice Loss\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "dice_loss = DiceLoss()\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Number of epochs to train\n",
    "num_epochs = 50\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# Best validation loss for checkpointing\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "def check_and_adjust_labels(labels, num_classes):\n",
    "    min_label = labels.min().item()\n",
    "    max_label = labels.max().item()\n",
    "\n",
    "    if min_label < 0 or max_label >= num_classes:\n",
    "        print(f\"Original label range: min {min_label}, max {max_label}\")\n",
    "\n",
    "        # Adjust labels if necessary\n",
    "        labels = torch.clamp(labels, min=0, max=num_classes - 1)\n",
    "        print(f\"Adjusted label range: min {labels.min().item()}, max {labels.max().item()}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Define the dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, label_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.nii.gz')]\n",
    "        self.label_files = [f for f in os.listdir(label_dir) if f.endswith('.nii.gz')]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        lbl_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "\n",
    "        try:\n",
    "            image = sitk.ReadImage(img_path)\n",
    "            label = sitk.ReadImage(lbl_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading files {img_path} or {lbl_path}: {e}\")\n",
    "            return None, None  # Or handle the case appropriately\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        image = sitk.GetArrayFromImage(image).astype(np.float32)\n",
    "        label = sitk.GetArrayFromImage(label).astype(np.float32)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        image = torch.tensor(image).unsqueeze(0)  # Add channel dimension\n",
    "        label = torch.tensor(label).long()  # Ensure label is of long type\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = CustomDataset('F:\\\\Repositories\\\\FLARE22Train\\\\normalised_images\\\\train', 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_labels\\\\train')\n",
    "val_dataset = CustomDataset('F:\\\\Repositories\\\\FLARE22Train\\\\normalised_images\\\\val', 'F:\\\\Repositories\\\\FLARE22Train\\\\resampled_labels\\\\val')\n",
    "\n",
    "# Use num_workers=0 for debugging\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Move model to device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        if images is None or labels is None:\n",
    "            continue\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)  # Move to device\n",
    "\n",
    "        # Check and adjust labels\n",
    "        labels = check_and_adjust_labels(labels, num_classes=4)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the losses\n",
    "        try:\n",
    "            loss_dice = dice_loss(outputs, labels)\n",
    "            loss_ce = cross_entropy_loss(outputs, labels)\n",
    "            loss = loss_dice + loss_ce\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError: {e}\")\n",
    "            print(f\"Output shape: {outputs.shape}\")\n",
    "            print(f\"Label shape: {labels.shape}\")\n",
    "            raise e\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_loss_list.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            if val_images is None or val_labels is None:\n",
    "                continue\n",
    "\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)  # Move to device\n",
    "\n",
    "            # Check and adjust labels\n",
    "            val_labels = check_and_adjust_labels(val_labels, num_classes=4)\n",
    "            \n",
    "            val_outputs = model(val_images)\n",
    "\n",
    "            val_loss_dice = dice_loss(val_outputs, val_labels)\n",
    "            val_loss_ce = cross_entropy_loss(val_outputs, val_labels)\n",
    "            val_loss += (val_loss_dice + val_loss_ce).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c500982-9ae6-4df9-a673-9ef8efb27ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
